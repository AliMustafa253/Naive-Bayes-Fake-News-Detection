{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "i18_0619.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmbC1nj6k6GL"
      },
      "source": [
        "#Ali Mustafa i18-0619_A Assignment 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtooNZXDnss3"
      },
      "source": [
        "Some Documentation points at the end of the file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMpfB_1XlC1s"
      },
      "source": [
        "#Initializing Train Test and Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BLcLY68lA8_"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# open data and misspelling.txt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnltXRColgRT",
        "outputId": "86a4cd3a-1875-4e5c-9622-075deb20adb5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T6TwgF8UDmU"
      },
      "source": [
        "def filter_text(text):\n",
        "  filter = '''!()#$-[]{}@~;:'\"\\,%^&*_<>./?'''\n",
        "\n",
        "  new_text = \"\"\n",
        "  for char in text:\n",
        "    if char not in filter:\n",
        "        new_text = new_text + char\n",
        "\n",
        "  return new_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ke17oUMlB_1"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/NLP/Assignment 5/stopwords-ur.txt', 'rt', encoding=\"utf-8\") as f:\n",
        "    reader = f.read()\n",
        "f.close()\n",
        "stop_words = reader.split()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxwgy6sSxCn-"
      },
      "source": [
        "folders = ['Fake','Real']\n",
        "Train = {}\n",
        "for c in folders:\n",
        "  Train[c] = []\n",
        "  i = 0\n",
        "  for file_name in os.listdir(r'/content/drive/My Drive/Colab Notebooks/NLP/Assignment 5/Train/{}/'.format(c)):\n",
        "    f = open ( r'/content/drive/My Drive/Colab Notebooks/NLP/Assignment 5/Train/{}/{}'.format(c,file_name) , 'rt' , encoding=\"utf-8-sig\" )\n",
        "    Train[c].append(filter_text(f.read() ))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2vRBpDAy9CM"
      },
      "source": [
        "Test = {}\n",
        "for c in folders:\n",
        "  Test[c] = []\n",
        "  i = 0\n",
        "  for file_name in os.listdir(r'/content/drive/My Drive/Colab Notebooks/NLP/Assignment 5/Test/{}/'.format(c)):\n",
        "    f = open ( r'/content/drive/My Drive/Colab Notebooks/NLP/Assignment 5/Test/{}/{}'.format(c,file_name) , 'rt' , encoding=\"utf-8-sig\" )\n",
        "    Test[c].append(filter_text(f.read() ))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkEM5jin_Tk6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "05e975bf-60b5-4f41-881f-c6c5c5fefeda"
      },
      "source": [
        "Train['Real'][0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'وسکانسن ویب ڈیسک  یہ بات ثابت ہوچکی ہے کہ دیرینہ زخم اور ناسور برقی سرگرمی سے جلدی ٹھیک ہوجاتے ہیں اسی بنا پر امریکی ماہرین نے ہلکی پھلکی پٹی بنائی ہے جو برقی جھماکوں سے زخم کو تیزی سے ٹھیک کرسکتی ہے یونیورسٹی آف وسکانسن میڈیسن کے سائنس دانوں نے ایک ایسی برقی بینڈیج بنائی ہے جسے پہنتے وقت الجھن نہیں ہوتی کیونکہ اس سے پہلے الیکٹرو تھراپی کی پٹیاں بہت بھاری بھرکم اور پیچیدہ ہوا کرتی تھیں اسی وجہ سے ماہرین نے یہ باسہولت برقی ڈریسنگ بنائی ہے جو مریض کے گھومنے پھرنے سے بجلی بناتی ہے جدید ڈریسنگ میں بہت باریک نینو جنریٹرز لگائے گئے ہیں اور ان سے ایک تار نکل کر پہننے والے کے بدن تک جاکر وہاں ایک پیوند سے منسلک ہوتا ہے جیسے جیسے مریض چلتا پھرتا ہے تو نینو جنریٹرز میں بجلی بنتی ہے اور بجلی کی لہروں سے دیرینہ ناسور تیزی سے مندمل ہونا شروع ہوجاتا ہے سانس لیتے ہوئے پسلیوں کے پھیلنے اور سکڑنے سے جو حرکت ہوتی ہے وہ نینو جنریٹر کے لیے بجلی کی تیاری میں بہت ہوتی ہے اس لیے پٹی کے لیے کسی بیٹری کی ضرورت نہیں ہوتی پٹی پر لگے برقیروں سے ہلکی بجلی زخم تک جاتی ہے اور کٹے پھٹے زخم کو تیزی سے مندمل کرتی ہے تجربہ گاہ میں چوہوں پر کیے گئے تجربات سے ثابت ہوا کہ جو زخم عام حالات میں 12 روزمیں درست ہوتے ہیں الیکٹریکل پٹی نے اسے تین دن میں ٹھیک کردیا'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smbCYPhdJbOk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "807b6dd1-8b0d-4667-ca64-c179c14ff14d"
      },
      "source": [
        "Test['Real'][0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'نیویارک اُردو پوائنٹ اخبارتازہ ترین 08 دسمبر2018ء امریکا میں سویابین کے نرخ پانچ ہفتے کی بلند ترین سطح پر رہے جس کی وجہ چین کے ساتھ تجارتی کشیدگی میں کمی کے باعث جنس کی برآمدات میں اضافے کی توقع ہے گندم اور مکئی کے نرخوں میں بھی اضافہ ریکارڈ کیا گیا شکاگو بورڈ آف ٹریڈمیں سویابین کے وقفے سے قبل جنوری کیلئے سودے 650 سینٹ بڑھ کر 916 ڈالر فی بشل طے پائے گندم کے مارچ کیلئے سودے 1525 سینٹ بڑھ کر 531 ڈالر فی بشل کے قریب طے پائے مکئی کے مارچ کیلئے سودے 225 سینٹ اضافے کے ساتھ 385 ڈالر فی بشل طے پائے'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD_ByrOoWChg"
      },
      "source": [
        "#Naive Baised Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcOLSYDmFfnh"
      },
      "source": [
        "Used to train both with conditions for boolean and stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kddH43CbFF7-",
        "outputId": "50e2df22-fc8d-4969-d315-677538cab49b"
      },
      "source": [
        "dicti = {}\n",
        "dicti['hello'] = 'hi'\n",
        "dicti['hello' + 'hi'] = 'Sup'\n",
        "dicti['hello' + 'hi']\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sup'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev9E4VE5fO7D"
      },
      "source": [
        "def extract_Vocab(training_set,stop_Bool = False):\n",
        "  unique_words = []\n",
        "  folders = ['Fake','Real']\n",
        "  for cl in folders:\n",
        "    for sentence in training_set[cl]:\n",
        "      tokenized = sentence.split()\n",
        "      for word in tokenized:\n",
        "        if stop_Bool == True:\n",
        "          if word not in unique_words and word not in stop_words:\n",
        "            unique_words.append(word)\n",
        "        else:\n",
        "          if word not in unique_words:\n",
        "            unique_words.append(word)\n",
        "  print(\"Length of unique words : \",len(unique_words))\n",
        "  return unique_words"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGXLqxBuBg0"
      },
      "source": [
        "def count_words(training_set):\n",
        "  length = 0\n",
        "  for sentence in training_set:\n",
        "      tokenized = sentence.split()\n",
        "      length += len(tokenized)\n",
        "\n",
        "  return length\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhVyx0XrUALA"
      },
      "source": [
        "def concatenate_text(training_set):\n",
        "  concatenate = {}\n",
        "  for sentences in training_set:\n",
        "    tokenized = sentences.split()\n",
        "    for word in tokenized:\n",
        "        if word not in concatenate:\n",
        "          concatenate[word] = 1\n",
        "        elif word in concatenate:\n",
        "          concatenate[word] += 1\n",
        "\n",
        "  return concatenate"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa8VIqY_RHsv"
      },
      "source": [
        "def count_token_words(concatenated_words,vocab):\n",
        "  dic_words = {}\n",
        "  for word in vocab:\n",
        "    if word in concatenated_words:\n",
        "      dic_words[word] = concatenated_words[word]\n",
        "    else:\n",
        "      dic_words[word] = 0\n",
        "\n",
        "  return dic_words\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtYp_7Gr1ZkU"
      },
      "source": [
        "def remove_Dupes(training_set):\n",
        "  unique_sentence = {}\n",
        "  array = []\n",
        "  for sentence in training_set:\n",
        "      new_sentence = \"\"\n",
        "      tokenized = sentence.split()\n",
        "      tokenized = list(dict.fromkeys(tokenized))\n",
        "      for word in tokenized:\n",
        "        new_sentence += word + \" \"\n",
        "      array.append(new_sentence)\n",
        "      # for word in tokenized:\n",
        "      #     if word not in unique_sentence:\n",
        "      #       unique_sentence[word] = 1\n",
        "      #       new_sentence += word + \" \"\n",
        "      # array.append(new_sentence)\n",
        "\n",
        "  return array"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP-SiA3AWEbB"
      },
      "source": [
        "def naives_BiasedTraining(training_set,boolean = False,stop_Bool = False):\n",
        "\n",
        "  vocab = extract_Vocab(training_set,stop_Bool) #unique words        #BOOL MAI HAR SENTENCE MAI UNIQUE WORD DEKHNA OK THX\n",
        "  folders = ['Fake','Real']\n",
        "  training_set_Class = {}\n",
        "\n",
        "  for cl in folders:\n",
        "    if boolean == True:\n",
        "      training_set_Class[cl] = copy.deepcopy(remove_Dupes(training_set[cl]))\n",
        "    else:\n",
        "      training_set_Class[cl] = copy.deepcopy(training_set[cl])\n",
        "\n",
        "  # print(training_set_Class[\"Fake\"])\n",
        "  # print(\"\\n\\nYOOO\\n\\n\",training_set[\"Fake\"])\n",
        "\n",
        "  conditional_prob = {}\n",
        "  prior = {}\n",
        "  N = len(training_set_Class['Real']) +len(training_set_Class['Fake'])\n",
        "\n",
        "  for cl in folders:\n",
        "    N_class = len(training_set_Class[cl])\n",
        "\n",
        "    N_words = count_words(training_set_Class[cl])\n",
        "    prior[cl] = float(N_class/N)\n",
        "    concatenated_words = concatenate_text(training_set_Class[cl])\n",
        "    # print(concatenated_sentences)\n",
        "    N_i = count_token_words(concatenated_words,vocab)\n",
        "    # print(N_i)\n",
        "    # print(\"PHULKI \",N_i['پھلکی'])\n",
        "    for w in vocab:\n",
        "      conditional_prob[w+cl] = (N_i[w] + 1) / (N_words + len(vocab))\n",
        "\n",
        "  return vocab,prior,conditional_prob\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISfjpjTaN3N"
      },
      "source": [
        "#NaiveBayesTest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCgkyl5rpHai"
      },
      "source": [
        "def extract_words_from_Text(vocab,test_sentence):\n",
        "  unique_words = []\n",
        "  tokenized = test_sentence.split()\n",
        "  for word in tokenized:\n",
        "    if word in vocab:\n",
        "      unique_words.append(word)\n",
        "  return unique_words"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Gb5c0--fRz"
      },
      "source": [
        "def remove_Dupes_Test(test_sentence):\n",
        "      unique_sentence = {}\n",
        "      new_sentence = \"\"\n",
        "      tokenized = test_sentence.split()\n",
        "      for word in tokenized:\n",
        "          if word not in unique_sentence:\n",
        "            unique_sentence[word] = 1\n",
        "            new_sentence += word + \" \"\n",
        "\n",
        "      return new_sentence"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8aUpLLEaZJb"
      },
      "source": [
        "import math\n",
        "import numpy\n",
        "\n",
        "def naive_BayesTest(training_set,vocab,prior,conditional_prob,test_sentence,boolean = False):\n",
        "  if boolean == True:\n",
        "    test_sentence = remove_Dupes_Test(test_sentence)\n",
        "  W = extract_words_from_Text(vocab,test_sentence)\n",
        "  folders = ['Fake','Real']\n",
        "  score = {}\n",
        "  for cl in folders:\n",
        "    score[cl] = math.log(prior[cl])\n",
        "    for w in W:\n",
        "      score[cl] += math.log(conditional_prob[w+cl])\n",
        "\n",
        "  # print(score['Fake'])\n",
        "  # print(score['Real'])\n",
        "  if (score['Fake'] > score['Real']):\n",
        "    return 'Fake'\n",
        "  else:\n",
        "    return 'Real'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVIhX7Los-pk"
      },
      "source": [
        "#OUTPUT DISPLAY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5wklFtRCf8y"
      },
      "source": [
        "##Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bka0D0tCbBD",
        "outputId": "7ffd69a2-a79a-4c66-dcbb-1d30ee319ab2"
      },
      "source": [
        "trained = naives_BiasedTraining(Train)    #3 values vocab,prior,cond_prob\n",
        "# print(trained[1]['Real'])\n",
        "# print(trained[1]['Fake'])\n",
        "\n",
        "# print(trained[2])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of unique words :  15374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YDuB-Xknadx",
        "outputId": "c1a25d52-f4d9-4a99-8536-7fec774fa952"
      },
      "source": [
        "folders = ['Fake','Real']\n",
        "total = len(Test['Real']+Test['Fake'])\n",
        "correct = 0\n",
        "confusion_Matrix = [ [0,0] , [0,0] ]    \n",
        "      # FAKE  Real\n",
        "# FAKE              \n",
        "# Real\n",
        "for cl in folders:\n",
        "\n",
        "  for doc in Test[cl]:\n",
        "    result = naive_BayesTest(Train,trained[0],trained[1],trained[2],doc,True)\n",
        "    if result == cl:\n",
        "      correct+=1\n",
        "\n",
        "    if result == 'Fake':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[0][0] += 1\n",
        "      else:\n",
        "        confusion_Matrix[1][0] += 1\n",
        "    elif result == 'Real':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[1][1] += 1\n",
        "      else:\n",
        "        confusion_Matrix[0][1] += 1\n",
        "\n",
        "print(\"Naive Bayes\")\n",
        "print(\"Total Test Set Length : \",total)\n",
        "print(\"Correct Guesses : \",correct)\n",
        "print(\"\\nTrue Positive : \",confusion_Matrix[1][1])\n",
        "print(\"False Positive : \",confusion_Matrix[0][1])\n",
        "print(\"True Negative : \",confusion_Matrix[0][0])\n",
        "print(\"False Negative : \",confusion_Matrix[1][0])\n",
        "\n",
        "nb_precision = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[0][1]))\n",
        "\n",
        "nb_recall = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[1][0]))\n",
        "\n",
        "nb_f1 = 2*((nb_precision*nb_recall)/(nb_precision+nb_recall))\n",
        "\n",
        "nb_precision = \"{:.2f}\".format(nb_precision*100)\n",
        "nb_recall = \"{:.2f}\".format(nb_recall*100)\n",
        "nb_f1 = \"{:.2f}\".format(nb_f1*100)\n",
        "\n",
        "print(\"\\nPrecision is : \",nb_precision)\n",
        "\n",
        "nb_accuracy = float(correct/total)\n",
        "nb_accuracy = \"{:.2f}\".format(nb_accuracy*100)\n",
        "print(\"Accuracy is : \",nb_accuracy)\n",
        "\n",
        "print(\"Recall is : \",nb_recall)\n",
        "print(\"F1 is : \",nb_f1)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes\n",
            "Total Test Set Length :  262\n",
            "Correct Guesses :  195\n",
            "\n",
            "True Positive :  118\n",
            "False Positive :  35\n",
            "True Negative :  77\n",
            "False Negative :  32\n",
            "\n",
            "Precision is :  77.12\n",
            "Accuracy is :  74.43\n",
            "Recall is :  78.67\n",
            "F1 is :  77.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJAbnvwDxoRJ"
      },
      "source": [
        "##Naive Bayes With Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ZfKyczxtIL",
        "outputId": "c56344e7-7d7f-4f0c-a610-0512652cb41c"
      },
      "source": [
        "trained_stop = naives_BiasedTraining(Train,False,True)    #3 values vocab,prior,cond_prob\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of unique words :  15213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtwjWU4kx6Bk",
        "outputId": "4961c6f6-67a6-430f-b567-6a9d8cadc6a0"
      },
      "source": [
        "folders = ['Fake','Real']\n",
        "total = len(Test['Real']+Test['Fake'])\n",
        "correct = 0\n",
        "confusion_Matrix = [ [0,0] , [0,0] ]    \n",
        "      # FAKE  Real\n",
        "# FAKE              \n",
        "# Real\n",
        "for cl in folders:\n",
        "\n",
        "  for doc in Test[cl]:\n",
        "    result = naive_BayesTest(Train,trained_stop[0],trained_stop[1],trained_stop[2],doc,True)\n",
        "    if result == cl:\n",
        "      correct+=1\n",
        "\n",
        "    if result == 'Fake':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[0][0] += 1\n",
        "      else:\n",
        "        confusion_Matrix[1][0] += 1\n",
        "    elif result == 'Real':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[1][1] += 1\n",
        "      else:\n",
        "        confusion_Matrix[0][1] += 1\n",
        "\n",
        "print(\"Naive Bayes with Stop Words\")\n",
        "print(\"Total Test Set Length : \",total)\n",
        "print(\"Correct Guesses : \",correct)\n",
        "print(\"\\nTrue Positive : \",confusion_Matrix[1][1])\n",
        "print(\"False Positive : \",confusion_Matrix[0][1])\n",
        "print(\"True Negative : \",confusion_Matrix[0][0])\n",
        "print(\"False Negative : \",confusion_Matrix[1][0])\n",
        "\n",
        "nbs_precision = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[0][1]))\n",
        "\n",
        "nbs_recall = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[1][0]))\n",
        "\n",
        "nbs_f1 = 2*((nbs_precision*nbs_recall)/(nbs_precision+nbs_recall))\n",
        "\n",
        "nbs_precision = \"{:.2f}\".format(nbs_precision*100)\n",
        "nbs_recall = \"{:.2f}\".format(nbs_recall*100)\n",
        "nbs_f1 = \"{:.2f}\".format(nbs_f1*100)\n",
        "\n",
        "print(\"\\nPrecision is : \",nbs_precision)\n",
        "\n",
        "nbs_accuracy = float(correct/total)\n",
        "nbs_accuracy = \"{:.2f}\".format(nbs_accuracy*100)\n",
        "print(\"Accuracy is : \",nbs_accuracy)\n",
        "\n",
        "print(\"Recall is : \",nbs_recall)\n",
        "print(\"F1 is : \",nbs_f1)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes with Stop Words\n",
            "Total Test Set Length :  262\n",
            "Correct Guesses :  191\n",
            "\n",
            "True Positive :  113\n",
            "False Positive :  34\n",
            "True Negative :  78\n",
            "False Negative :  37\n",
            "\n",
            "Precision is :  76.87\n",
            "Accuracy is :  72.90\n",
            "Recall is :  75.33\n",
            "F1 is :  76.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfxO4OMP0lYB"
      },
      "source": [
        "##Boolean Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU8HHDTv9WTQ",
        "outputId": "dc45d25d-1d9a-4649-d9e8-9b69c102cc98"
      },
      "source": [
        "bool_trained = naives_BiasedTraining(Train,True)    #3 values vocab,prior,cond_prob"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of unique words :  15374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWiiIzh_-B5V",
        "outputId": "b6528db7-076f-4d76-8dd1-73161eaa7a16"
      },
      "source": [
        "folders = ['Fake','Real']\n",
        "total = len(Test['Real']+Test['Fake'])\n",
        "correct = 0\n",
        "confusion_Matrix = [ [0,0] , [0,0] ]    \n",
        "      # FAKE  Real\n",
        "# FAKE              \n",
        "# Real\n",
        "for cl in folders:\n",
        "\n",
        "  for doc in Test[cl]:\n",
        "    result = naive_BayesTest(Train,bool_trained[0],bool_trained[1],bool_trained[2],doc,True)\n",
        "    if result == cl:\n",
        "      correct+=1\n",
        "\n",
        "    if result == 'Fake':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[0][0] += 1\n",
        "      else:\n",
        "        confusion_Matrix[1][0] += 1\n",
        "    elif result == 'Real':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[1][1] += 1\n",
        "      else:\n",
        "        confusion_Matrix[0][1] += 1\n",
        "\n",
        "print(\"Boolean Naive Bayes without Stop Words\")\n",
        "print(\"Total Test Set Length : \",total)\n",
        "print(\"Correct Guesses : \",correct)\n",
        "print(\"\\nTrue Positive : \",confusion_Matrix[1][1])\n",
        "print(\"False Positive : \",confusion_Matrix[0][1])\n",
        "print(\"True Negative : \",confusion_Matrix[0][0])\n",
        "print(\"False Negative : \",confusion_Matrix[1][0])\n",
        "\n",
        "boolean_nb_precision = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[0][1]))\n",
        "\n",
        "boolean_nb_recall = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[1][0]))\n",
        "\n",
        "boolean_nb_f1 = 2*((boolean_nb_precision*boolean_nb_recall)/(boolean_nb_precision+boolean_nb_recall))\n",
        "\n",
        "boolean_nb_precision = \"{:.2f}\".format(boolean_nb_precision*100)\n",
        "boolean_nb_recall = \"{:.2f}\".format(boolean_nb_recall*100)\n",
        "boolean_nb_f1 = \"{:.2f}\".format(boolean_nb_f1*100)\n",
        "\n",
        "print(\"\\nPrecision is : \",boolean_nb_precision)\n",
        "boolean_nb_accuracy = float(correct/total)\n",
        "boolean_nb_accuracy = \"{:.2f}\".format(boolean_nb_accuracy*100)\n",
        "print(\"Accuracy is : \",boolean_nb_accuracy)\n",
        "\n",
        "print(\"Recall is : \",boolean_nb_recall)\n",
        "print(\"F1 is : \",boolean_nb_f1)\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boolean Naive Bayes without Stop Words\n",
            "Total Test Set Length :  262\n",
            "Correct Guesses :  201\n",
            "\n",
            "True Positive :  128\n",
            "False Positive :  39\n",
            "True Negative :  73\n",
            "False Negative :  22\n",
            "\n",
            "Precision is :  76.65\n",
            "Accuracy is :  76.72\n",
            "Recall is :  85.33\n",
            "F1 is :  80.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aaFKmjG-Kl7"
      },
      "source": [
        "##Boolean Naive Bayes with Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "768_cipH-Vme",
        "outputId": "be96242c-e245-483e-f58c-1c8448e8c84b"
      },
      "source": [
        "bool_trained_stop = naives_BiasedTraining(Train,True,True)    #3 values vocab,prior,cond_prob"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of unique words :  15213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M09k35ma-6Sv",
        "outputId": "c06ed0f0-a1d6-4edd-8d97-f0da43903b1b"
      },
      "source": [
        "folders = ['Fake','Real']\n",
        "total = len(Test['Real']+Test['Fake'])\n",
        "correct = 0\n",
        "confusion_Matrix = [ [0,0] , [0,0] ]    \n",
        "      # FAKE  Real\n",
        "# FAKE              \n",
        "# Real\n",
        "for cl in folders:\n",
        "\n",
        "  for doc in Test[cl]:\n",
        "    result = naive_BayesTest(Train,bool_trained_stop[0],bool_trained_stop[1],bool_trained_stop[2],doc,True)\n",
        "    if result == cl:\n",
        "      correct+=1\n",
        "\n",
        "    if result == 'Fake':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[0][0] += 1\n",
        "      else:\n",
        "        confusion_Matrix[1][0] += 1\n",
        "    elif result == 'Real':\n",
        "      if result == cl:\n",
        "        confusion_Matrix[1][1] += 1\n",
        "      else:\n",
        "        confusion_Matrix[0][1] += 1\n",
        "\n",
        "print(\"Boolean Naive Bayes with Stop Words\")\n",
        "print(\"Total Test Set Length : \",total)\n",
        "print(\"Correct Guesses : \",correct)\n",
        "print(\"\\nTrue Positive : \",confusion_Matrix[1][1])\n",
        "print(\"False Positive : \",confusion_Matrix[0][1])\n",
        "print(\"True Negative : \",confusion_Matrix[0][0])\n",
        "print(\"False Negative : \",confusion_Matrix[1][0])\n",
        "\n",
        "boolean_nbs_precision = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[0][1]))\n",
        "\n",
        "boolean_nbs_recall = float((confusion_Matrix[1][1])/(confusion_Matrix[1][1]+confusion_Matrix[1][0]))\n",
        "\n",
        "boolean_nbs_f1 = 2*((boolean_nbs_precision*boolean_nbs_recall)/(boolean_nbs_precision+boolean_nbs_recall))\n",
        "\n",
        "boolean_nbs_precision = \"{:.2f}\".format(boolean_nbs_precision*100)\n",
        "boolean_nbs_recall = \"{:.2f}\".format(boolean_nbs_recall*100)\n",
        "boolean_nbs_f1 = \"{:.2f}\".format(boolean_nbs_f1*100)\n",
        "\n",
        "print(\"\\nPrecision is : \",boolean_nbs_precision)\n",
        "boolean_nbs_accuracy = float(correct/total)\n",
        "boolean_nbs_accuracy = \"{:.2f}\".format(boolean_nbs_accuracy*100)\n",
        "print(\"Accuracy is : \",boolean_nbs_accuracy)\n",
        "print(\"Recall is : \",boolean_nbs_recall)\n",
        "print(\"F1 is : \",boolean_nbs_f1)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Boolean Naive Bayes with Stop Words\n",
            "Total Test Set Length :  262\n",
            "Correct Guesses :  197\n",
            "\n",
            "True Positive :  126\n",
            "False Positive :  41\n",
            "True Negative :  71\n",
            "False Negative :  24\n",
            "\n",
            "Precision is :  75.45\n",
            "Accuracy is :  75.19\n",
            "Recall is :  84.00\n",
            "F1 is :  79.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQk-y6hKF8jT"
      },
      "source": [
        "#Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPswZMd3GC0X",
        "outputId": "2120dfd2-5211-4086-c0d6-a267e25a93c2"
      },
      "source": [
        "print(\"Naive Bayes \\nPrecision is       : \",nb_precision)\n",
        "print(\"Accuracy is        : \",nb_accuracy)\n",
        "print(\"Recall is          : \",nb_recall)\n",
        "print(\"F1 is              : \",nb_f1)\n",
        "\n",
        "print(\"\\nNaive Bayes with Stop Words \\nPrecision is       : \",nbs_precision)\n",
        "print(\"Accuracy is        : \",nbs_accuracy)\n",
        "print(\"Recall is          : \",nbs_recall)\n",
        "print(\"F1 is              : \",nbs_f1)\n",
        "\n",
        "print(\"\\nBoolean Naive Bayes \\nPrecision is       : \",boolean_nb_precision)\n",
        "print(\"Accuracy is        : \",boolean_nb_accuracy)\n",
        "print(\"Recall is          : \",boolean_nb_recall)\n",
        "print(\"F1 is              : \",boolean_nb_f1)\n",
        "\n",
        "print(\"\\nBoolean Naive Bayes with Stop Words \\nPrecision is       : \",boolean_nbs_precision)\n",
        "print(\"Accuracy is        : \",boolean_nbs_accuracy)\n",
        "print(\"Recall is          : \",boolean_nbs_recall)\n",
        "print(\"F1 is              : \",boolean_nbs_f1)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes \n",
            "Precision is       :  77.12\n",
            "Accuracy is        :  74.43\n",
            "Recall is          :  78.67\n",
            "F1 is              :  77.89\n",
            "\n",
            "Naive Bayes with Stop Words \n",
            "Precision is       :  76.87\n",
            "Accuracy is        :  72.90\n",
            "Recall is          :  75.33\n",
            "F1 is              :  76.09\n",
            "\n",
            "Boolean Naive Bayes \n",
            "Precision is       :  76.65\n",
            "Accuracy is        :  76.72\n",
            "Recall is          :  85.33\n",
            "F1 is              :  80.76\n",
            "\n",
            "Boolean Naive Bayes with Stop Words \n",
            "Precision is       :  75.45\n",
            "Accuracy is        :  75.19\n",
            "Recall is          :  84.00\n",
            "F1 is              :  79.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi3VcnLpGtYq"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Expects all features to be independent.\n",
        "*   Order of sentences also important in analysing sentiment. This is disturbed with Boolean and stop words\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Key Points in using Stop Words**\n",
        "\n",
        "\n",
        "\n",
        "*   Stop Words refer to the helping words that are used in structuring etc. They don't have much of a role in the identification. So they can act as an hinderance to the overall accuracy in the calculation of precision of a dataset.\n",
        "*   In this case however, Adding Stop words is reducing the accuracy. This could just be inaccuracy in the dataset in general due to not efficient/accurate/or large enough data available and it is having more precision by luck.\n",
        "\n",
        "**Boolean Naive Bayes**\n",
        "\n",
        "\n",
        "\n",
        "*   Makes it so that each document only has one occurance of the word. Making it so that you dont get more than one count from one document for a particular word.\n",
        "*   Generally makes it more accurate. \n",
        "*   Thus Improves the performance in our case for both stop and no stop words.\n",
        "*   Removing Stop words reduced accuracy. In all functions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}